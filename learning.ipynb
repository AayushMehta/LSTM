{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK4CEvrt_DVn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "def read_data(vib):\n",
        "  dat=pd.read_csv(vib)\n",
        "  sensorname=dat.keys()[2:-1] \n",
        "  return dat, sensorname\n",
        "\n",
        "\n",
        "def explore(data):\n",
        "  print('Data overview: ')\n",
        "  print(data.shape); print()\n",
        "  print('keys :') ; print(data.keys()); print()\n",
        "  print( 'status options: ');  print( data['machine_status'].unique()); print()\n",
        "  print (data['machine_status'].value_counts()); print()\n",
        "  info=data.describe()\n",
        "  varianz=pd.DataFrame({'var':data.var()})\n",
        "  info=pd.concat([info,varianz.transpose()])\n",
        "  return data.head(), data.tail(), info\n",
        "\n",
        "\n",
        "def manipulate_X(data, printplot=False):\n",
        "    data=data.drop(labels=['sensor_15'],axis=1)\n",
        "    data=data.drop(labels=['sensor_00'],axis=1)\n",
        "\n",
        "    data['sensor_51'][110000:140000]=data['sensor_50'][110000:140000] \n",
        "    data=data.drop(labels=['sensor_50'],axis=1)\n",
        "\n",
        "\n",
        "    data=data.drop(labels=['sensor_06','sensor_07','sensor_08','sensor_09'],axis=1)\n",
        "    data=data.fillna(method=\"pad\",limit=30)\n",
        "    data=data.dropna()\n",
        "    if printplot==True:\n",
        "        print((data.isna().sum()))\n",
        "        plotting_stuff((data.isna().sum()[2:-1]),'bar','fill_nan',saving=True)\n",
        "        \n",
        "    return data\n",
        "\n",
        "def preprocess(data):\n",
        "    from sklearn import preprocessing\n",
        "\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(data)\n",
        "    encoded_y=le.transform(data)\n",
        "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(le_name_mapping)\n",
        "    return pd.DataFrame(encoded_y,columns=['target'])\n",
        "\n",
        "#call series_to_supervise() to create shifted data.\n",
        "#\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, namen = list(),list()\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        namen +=[('sensor%d(t-%d)' %(j+1, i)) for j in range (n_vars)]\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            namen +=[('sensor%d(t)' %(j+1)) for j in range (n_vars)]\n",
        "        else:\n",
        "            namen +=[('sensor%d(t+%d)' '%'(j+1, i)) for j in range (n_vars)]\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns=namen\n",
        "\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "# splitting Data into Train Validation Test Sets\n",
        "def splitting_and_shape_data(data_x,data_y):    \n",
        "    train_X=data_x[0:120000].values\n",
        "    train_Y=data_y[0:120000].values\n",
        "    \n",
        "    val_X=data_x[140000::].values\n",
        "    val_Y=data_y[140000::].values\n",
        "    \n",
        "    test_X=data_x[120000:140000].values\n",
        "    test_Y=data_y[120000:140000].values\n",
        "      \n",
        "    train_X.astype('float32')\n",
        "    val_X.astype('float32')\n",
        "    test_X.astype('float32')\n",
        "    \n",
        "    return train_X,train_Y,val_X,val_Y,test_X,test_Y,    \n",
        "#creating perticular 3 shape for LSTM sample timestap and dataset features\n",
        "def reshape_for_Lstm(data):    \n",
        "    timesteps=1\n",
        "    samples=int(np.floor(data.shape[0]/timesteps))\n",
        "\n",
        "    data=data.reshape((samples,timesteps,data.shape[1]))       \n",
        "    return data\n",
        "\n",
        "#interpreting data class correctly\n",
        "def one_hot(train_Y,val_Y,test_Y):    \n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    \n",
        "    oneHot=OneHotEncoder()\n",
        "    oneHot.fit(train_Y.reshape(-1,1))\n",
        "    \n",
        "    train_Y_Hot=oneHot.transform(train_Y.reshape(-1,1)).toarray()\n",
        "    val_Y_Hot  =oneHot.transform(val_Y.reshape(-1,1)).toarray()\n",
        "    test_Y_Hot =oneHot.transform(test_Y.reshape(-1,1)).toarray()\n",
        "    \n",
        "    return train_Y_Hot,val_Y_Hot,test_Y_Hot\n",
        "\n",
        "# using LSTM model neuron 75 activation fn relu used for probablity distribution\n",
        "#of classification\n",
        "def model_setup_seq(in_shape):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import LSTM\n",
        "    from tensorflow.keras.layers import Dropout\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(LSTM(75,activation='relu', input_shape=(in_shape[1],in_shape[2]), \n",
        "                   return_sequences=True)  )\n",
        "\n",
        "    model.add(LSTM(56,activation='relu')) # 56 layer used\n",
        "    model.add(Dense(1)) # for 1 neuron prediction\n",
        "    \n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "#softmax to normalize output over probablity distribution\n",
        "#training the model    \n",
        "def model_setup_Fapi(in_shape):\n",
        "    from tensorflow.keras.layers import LSTM\n",
        "    from tensorflow.keras.layers import Dropout\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    \n",
        "    inputs= tf.keras.Input(shape=(in_shape[1],in_shape[2]))\n",
        "    x=LSTM(2048,activation='relu', input_shape=(in_shape[1],in_shape[2]),return_sequences=True)(inputs)\n",
        "    x=LSTM(2048,activation='relu')(x)\n",
        "    out_signal=Dense(1, name='signal_out')(x)\n",
        "    out_class=Dense(3,activation='softmax', name='class_out')(x)\n",
        "    \n",
        "    model=tf.keras.Model(inputs=inputs, outputs=[out_signal,out_class])\n",
        "    \n",
        "    model.compile(loss={'signal_out':'mean_squared_error',\n",
        "                        'class_out' :'categorical_crossentropy'},\n",
        "                         optimizer='adam',\n",
        "                         metrics={'class_out':'acc'})\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n",
        "def plot_training(history,what='loss',saving=False,name='training'):\n",
        "    fig=plt.figure()\n",
        "    plt.plot(history[0])\n",
        "    plt.plot(history[1])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'])\n",
        "    if what=='loss':\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "    elif what=='acc':   \n",
        "        plt.title('model Acc')\n",
        "        plt.ylabel('Accuracy')   \n",
        "    if saving==True:\n",
        "        fig.savefig( name +'_'+ what + '.png', format='png', dpi=300, transparent=True)\n",
        "\n",
        "\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'])\n",
        "    if saving==True:\n",
        "        fig.savefig( name +'_ACC.png', format='png', dpi=300, transparent=True)  \n",
        "    plt.show()\n",
        "    \n",
        "def plot_signal_hat(Y_hat,Y_test,saving=False,name='results_signal'):\n",
        "    fig= plt.figure()\n",
        "    plt.plot(Y_hat)\n",
        "    plt.plot(Y_test)\n",
        "    plt.legend(['target','target_predicted'])\n",
        "    plt.ylabel('Zustand')\n",
        "    plt.title('Pediction on test data')\n",
        "    if saving==True:\n",
        "        fig.savefig( name +'.png', format='png', dpi=300, transparent=True)\n",
        "    plt.show()\n",
        "        \n",
        "def plot_class_hat(Y_hat,Y_test,saving=False,name='results_class'):   \n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib import cm\n",
        "    import numpy as np\n",
        "    x=np.linspace(1,len(Y_hat),len(Y_hat))\n",
        "\n",
        "    fig=plt.figure()\n",
        "    ax1=fig.add_subplot(111)\n",
        "    ax1=plt.plot(x,Y_test)\n",
        "    ax1=plt.scatter(x,Y_hat,c=cm.hot(np.abs(Y_hat)), edgecolor='none') \n",
        "    plt.legend(['target','target_predicted'])\n",
        "    if saving==True:\n",
        "        fig.savefig( name +'.png', format='png', dpi=300, transparent=True)\n",
        "    plt.show()\n",
        "       \n",
        "   \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "   \n",
        "    data,sensorname=read_data('/content/drive/MyDrive/vib.csv')\n",
        "\n",
        " \n",
        "    encoded_y=preprocess(data['machine_status'])\n",
        "    Values=pd.concat([data[sensorname],encoded_y],axis=1)\n",
        "\n",
        " \n",
        "    Values=manipulate_X(Values, printplot=False); sensorname=Values.keys()[:-1] \n",
        "\n",
        "\n",
        "    Future=1\n",
        "\n",
        "    data_win=series_to_supervised(Values, n_in=Future, n_out=1)\n",
        "    to_remove_list =['sensor'+str(n)+'(t)' for n in range(1,len(Values.columns)+1)] #now remove all non shifted elements again. so we retreive elements and shifted target\n",
        "\n",
        "    data_y=data_win.iloc[:,-1] \n",
        "    data_x=data_win.drop(to_remove_list, axis=1) \n",
        "    data_x.drop(data_x.columns[len(data_x.columns)-1], axis=1, inplace=True)\n",
        "    \n",
        "\n",
        "    \n",
        "    train_X,train_Y,val_X,val_Y,test_X,test_Y=splitting_and_shape_data(data_x,data_y)\n",
        "    train_Y_Hot,val_Y_Hot,test_Y_Hot=one_hot(train_Y,val_Y,test_Y)\n",
        "\n",
        "    scaler=MinMaxScaler().fit(train_X)\n",
        "    train_X=scaler.transform(train_X) \n",
        "   \n",
        "    scaler=MinMaxScaler().fit(val_X)\n",
        "    val_X=scaler.transform(val_X)  \n",
        "    \n",
        "    scaler=MinMaxScaler().fit(test_X)\n",
        "    test_X=scaler.transform(test_X)  \n",
        "\n",
        "    train_X=reshape_for_Lstm(train_X)\n",
        "    val_X=reshape_for_Lstm(val_X)\n",
        "    test_X=reshape_for_Lstm(test_X)\n",
        "\n",
        "    \n",
        "    Train=True\n",
        "    inputshape_X=(train_X.shape)\n",
        "  \n",
        "    \n",
        "    if Train==True:\n",
        "\n",
        "    #epocs defined here\n",
        "        model=model_setup_Fapi(inputshape_X)\n",
        "        history = model.fit(train_X, [train_Y, train_Y_Hot], epochs=400, batch_size=2048, validation_data=(val_X, [val_Y,val_Y_Hot]), shuffle=False)\n",
        "        plot_training([history.history['class_out_loss'],history.history['val_class_out_loss']],\n",
        "                      what='loss',\n",
        "                      saving=True,\n",
        "                      name=('training_'+ str(Future)))  \n",
        "        plot_training([history.history['class_out_acc'],history.history['val_class_out_acc']],\n",
        "                      what='acc',\n",
        "                      saving=True,\n",
        "                      name=('training_'+ str(Future))) \n",
        "        model.save('./model/Pump_LSTM_Fapi_4_'+ str(Future))\n",
        " \n",
        "    else:  \n",
        "        model=tf.keras.models.load_model('./model/Pump_LSTM_Fapi_4_1')\n",
        "\n",
        "    [yhat,yclass] = model.predict(test_X)\n",
        "    print(yhat)\n",
        "    Yclass=[np.argmax(yclass[i],0) for i in range(len(yclass))] # get final class\n",
        "    Y_HOT = [np.argmax(test_Y_Hot[i],0) for i in range(len(test_Y_Hot))] \n",
        "    plot_signal_hat(test_Y, yhat,saving=True, name='Prediction_Signal_fapi3_42_'+ str(Future))\n",
        "    plot_signal_hat(Yclass, Y_HOT ,saving=True, name='Prediction_class_fapi3_42_'+ str(Future))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5IwsHdta6WYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "OBytQTxsawlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "hknokkNSbWNj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xs4Q_LQpADQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}